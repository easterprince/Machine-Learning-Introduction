{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "## Открытый курс по машинному обучению.\n",
    "<center> Автор материала: Павел Нестеров (@mephistopheies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашняя работа №4\n",
    "## <center> Логистическая регрессия в задаче тегирования вопросов StackOverflow\n",
    "\n",
    "**Надо вывести формулы, где это просится (да, ручка и бумажка), заполнить код в клетках и выбрать ответы в [веб-форме](https://docs.google.com/forms/d/1dx3v1HMjVIiS9ixnXzlIm7jDb-iDYNlCHMe224w2o8E/edit?usp=sharing).**\n",
    "\n",
    "## 0. Описание задачи\n",
    "\n",
    "В этой домашней работе мы с вами изучим и запрограммируем модель для прогнозирования тегов по тексту вопроса на базе многоклассовой логистической регрессии. В отличие от обычной постановки задачи классификации (multiclass), в данном случае один пример может принадлежать одновременно к нескольким классам (multilabel). Мы будем реализовывать онлайн-версию алгоритма multilabel-классификации.\n",
    "\n",
    "Мы будем использовать небольшую выборку из протеггированных вопросов с сайта StackOverflow размером в 125 тысяч примеров (около 150 Мб, скачайте по [этой](https://drive.google.com/open?id=0B4bl7YMqDnViYVo0V2FubFVhMFE) ссылке).\n",
    "\n",
    "PS: Можно показать, что такая реализация совсем не эффективная и проще было бы использовать векторизированные вычисления. Для данного датасета так и есть. Но на самом деле подобные реализации используются в жизни, но естественно, написаны они не на Python. Например, в онлайн-моделях прогнозирования [CTR](https://en.wikipedia.org/wiki/Click-through_rate) юзеру показывается баннер, затем в зависимости от наличия клика происходит обновление параметров модели. В реальной жизни параметров модели может быть несколько сотен миллионов, а у юзера из этих ста миллионов от силы сто или тысяча параметров отличны от нуля, векторизировать такие вычисления не очень эффективно. Обычно все это хранится в огромных кластерах в in-memory базах данных, а обработка пользователей происходит распределенно.\n",
    "\n",
    "PS2:\n",
    "- в процессе решения домашней работы вам придется работать с текстом, и у вас может возникнуть желание сделать очевидный препроцессинг, например привести все слова в нижний регистр, в-общем **этого делать не нужно, если не оговорено заранее в задании**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем версии используемых библиотек. Совпадут ли ответы в случае других версий - не гарантируется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.7.4\n",
      "IPython 7.8.0\n",
      "\n",
      "numpy 1.16.5\n",
      "scipy 1.3.1\n",
      "pandas 0.25.1\n",
      "matplotlib 3.1.1\n",
      "sklearn 0.21.3\n",
      "\n",
      "compiler   : MSC v.1915 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "Git hash   : 707b8bbdbfab0a798cff966c03c7b6b5834f36d4\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,sklearn -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# поменяйте на свой путь\n",
    "DS_FILE_NAME = '../../../data/stackoverflow_sample_125k.tsv'\n",
    "TAGS_FILE_NAME = '../../../data/top10_tags.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'javascript', 'python', 'c#', 'php', 'jquery', 'ios', 'android', 'c++', 'html', 'java'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "top_tags = set(top_tags)\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Многоклассовая логистическая регрессия\n",
    "\n",
    "Вспомним, как получается логистическая регрессия для двух классов $\\left\\{0, 1\\right\\}$, вероятность принадлежности объекта к классу $1$ выписывается по теореме Байеса:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\textbf{x}\\right) &=& \\dfrac{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\textbf{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} = \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\textbf{x}$ – вектор признаков объекта\n",
    "- $\\sigma$ – обозначение функции логистического сигмоида при скалярном аргументе\n",
    "- $a = \\log \\frac{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\textbf{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x_i$ – это отношение мы моделируем линейной функцией от признаков объекта и параметров модели\n",
    "\n",
    "Данное выражение легко обобщить до множества из $K$ классов, изменится только знаменатель в формуле Байеса. Запишем вероятность принадлежности объекта к классу $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\textbf{x}\\right) &=& \\dfrac{p\\left(\\textbf{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\textbf{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} = \\sigma_k\\left(\\textbf{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – обозначение функции softmax при векторном аргументе\n",
    "- $z_k = \\log p\\left(\\textbf{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x_i$ – это выражение моделируется линейной функцией от признаков объекта и параметров модели для класса $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для моделирования полного правдоподобия примера мы используем [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution), а лучше его логарифм (для удобства):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\textbf{x}, y}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\textbf{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\textbf{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Получается хорошо знакомая нам функция [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (если домножить на $-1$). Правдоподобие нужно максимизировать, а, соответственно, перекрестную энтропию нужно минимизировать. Продифференцировав по параметрам модели, мы _легко_ получим правила обновления весов для градиентного спуска, **проделайте этот вывод, если вы его не делали** (если вы вдруг сдались, то на [этом](https://www.youtube.com/watch?v=-WiR16raQf4) видео есть разбор вывода, понимание этого вам понадобится для дальнейшего выполнения задания; если предпочитаете текст, то и он есть [тут](https://www.ics.uci.edu/~pjsadows/notes.pdf) и [тут](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\textbf{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "В стандартной формулировке получается, что вектор $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^K \\sigma_i = 1$. Но в нашей постановке задачи каждый пример может иметь несколько тегов или одновременно принадлежать к нескольким классам. Для этого мы немного изменим модель:\n",
    "- будем считать, что все теги независимы друг от друга, т.е. каждый исход – это логистическая регрессия на два класса (либо есть тег, либо его нет), тогда вероятность наличия тега у примера запишется следующим образом (каждый тег/класс как и в многоклассовой логрегрессии имеет свой набор параметров):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\textbf{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- наличие каждого тега мы будем моделировать с помощью <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">распределения Бернулли</a>\n",
    "\n",
    "<font color=\"red\">Вопрос 1.</font> Ваше первое задание –  записать упрощенное выражение логарифма правдоподобия примера с признаками $\\textbf{x}$. Как правило, многие алгоритмы оптимизации имеют интерфейс для минимизации функции, мы последуем этой же традиции и домножим полученное выражение на $-1$, а во второй части выведем формулы для минимизации полученного выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$ <---\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вывод формулы обновления весов\n",
    "\n",
    "<font color=\"red\">Вопрос 2.</font> В качестве второго задания вам предоставляется возможность вывести формулу градиента для $-\\mathcal{L}$. Какой вид она будет иметь?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$ <---\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Реализация базовой модели\n",
    "\n",
    "Вам предлагается каркас класса модели, разберите его внимательно, обращайте внимание на комментарии. Затем заполните пропуски, запустите полученную модель и ответьте на проверочный вопрос.\n",
    "\n",
    "Как вы могли уже заметить, при обновлении веса $w_{km}$ используется значение признака $x_m$, который равен $0$, если слова с индексом $m$ нет в предложении, и больше нуля, если такое слово есть. В нашем случае, чтобы не пересчитывать [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) самим или с помощью [sklearn.feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), мы будем идти по словам предложения в порядке их следования. Если какое-то слово встречается несколько раз, то мы добавляем его в аккумулятор со своим весом. В итоге получится то же самое, как если сначала посчитать количество одинаковых слов и домножить на соответствующий вес. Соответственно, при вычислении линейной комбинации $z$ весов модели и признаков примера необходимо учитывать только ненулевые признаки объекта.\n",
    "\n",
    "Подсказка:\n",
    "- если реализовывать вычисление сигмоида так же, как в формуле, то при большом отрицательном значении $z$ вычисление $e^{-z}$ превратится в очень большое число, которое вылетит за допустимые пределы\n",
    "- в то же время $e^{-z}$ от большого положительного $z$ будет нулем\n",
    "- воспользуйтесь свойствами функции $\\sigma$ для того, чтобы пофиксить эту ошибку и реализовать $\\sigma$ без риска overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        word_idx = self._vocab[word]\n",
    "                        z += self._w[tag][word_idx]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    sigma = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    log_arg = max(tolerance, sigma if y == 1 else 1 - sigma)\n",
    "                    sample_loss += -np.log(log_arg)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        dLdw = y - sigma\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate * dLdw\n",
    "                        self._b[tag] -= -learning_rate * dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fefa92c4b8c4d72ac2d3ac931e1baa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим эксемпляр модели и пройдемся по датасету\n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, действительно ли значение отрицательного логарифмического правдоподобия уменьшалось. Так как мы используем стохастический градентный спуск, не стоит ожидать плавного падения функции ошибки. Мы воспользуемся скользящим средним с окном в 10 тысяч примеров, чтобы хоть как-то сгладить график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the loss function on the last 10k train samples: 19.73\n"
     ]
    }
   ],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 3.</font>\n",
    "Вычислите среднее значение функции стоимости на последних 10 000 примеров тренировочного набора, к какому из значений ваш ответ ближе всего?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. 19.74 <---\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "В базовой модели первые 100 000 строк используются для обучения, а оставшиеся – для тестирования. Как вы можете заметить, значение отрицательного логарифмического правдоподобия не очень информативно, хоть и позволяет сравнивать разные модели. В качестве четвертого задания вам необходимо модифицировать базовую модель таким образом, чтобы метод `iterate_file` возвращал значение _точности_ на тестовой части набора данных. \n",
    "\n",
    "Точность определим следующим образом:\n",
    "- считаем, что тег у вопроса присутствует, если спрогнозированная вероятность тега больше 0.9\n",
    "- точность одного примера расчитывается как [коэффициент Жаккара](https://ru.wikipedia.org/wiki/Коэффициент_Жаккара) между множеством настоящих тегов и предсказанных моделью\n",
    "  - например, если у примера настоящие теги ['html', 'jquery'], а по версии модели ['ios', 'html', 'java'], то коэффициент Жаккара будет равен |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- метод `iterate_file` возвращает **среднюю** точность на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        self._acc = []\n",
    "        n = 0\n",
    "        test_acc = 0.0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "                \n",
    "                # количество определённых тегов, количество совпавших тегов\n",
    "                identified = 0\n",
    "                overlapped = 0                \n",
    "                \n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        word_idx = self._vocab[word]\n",
    "                        z += self._w[tag][word_idx]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    sigma = 1 / (1 + np.exp(-z))\n",
    "            \n",
    "            \n",
    "                    # обновляем количества определённых и совпавших тегов\n",
    "                    if sigma > 0.9:\n",
    "                        identified += 1\n",
    "                        if y == 1:\n",
    "                            overlapped += 1\n",
    "    \n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    log_arg = max(tolerance, sigma if y == 1 else 1 - sigma)\n",
    "                    sample_loss += -np.log(log_arg)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        dLdw = y - sigma\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate * dLdw\n",
    "                        self._b[tag] -= -learning_rate * dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "                \n",
    "                # вычисляем точность\n",
    "                sample_acc = overlapped / (identified + len(tags) - overlapped)\n",
    "                if n >= top_n_train:\n",
    "                    test_acc += sample_acc\n",
    "                self._acc.append(sample_acc)\n",
    "        \n",
    "        # завершаем вычисление точности для тестовой выборки\n",
    "        test_acc /= (n - top_n_train)\n",
    "        return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775511995bdf473dbde3025d5839114d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.58\n"
     ]
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# выведем полученное значение с точностью до двух знаков\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 4.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59 <---\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-регуляризация\n",
    "\n",
    "В качестве пятого задания вам необходимо добавить в класс `LogRegressor` поддержку $L_2$-регуляризации. В методе `iterate_file` должен появиться параметр `lmbda=0.01` со значением по умолчанию. С учетом регуляризации новая функция стоимости примет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(\\textbf W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Градиент первого члена суммы мы уже вывели, а для второго он имеет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(\\textbf W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "Если мы на каждом примере будем делать честное обновление всех весов, то все очень замедлится, ведь нам придется на каждой итерации пробегать по всем словам словаря. В ущерб теоретической корректности мы используем грязный трюк: будем регуляризировать только те слова, которые присутствуют в текущем предложении. Не забывайте, что смещение (bias) не регуляризируется. `sample_loss` тоже должен остаться без изменений.\n",
    "\n",
    "Замечание:\n",
    "- не забудьте, что нужно учитывать регуляризацию слова в градиентном шаге только один раз\n",
    "- условимся, что учитываем регуляризацию только при первой встрече слова\n",
    "- если бы мы считали сначала bag-of-words, то мы бы в цикле шли по уникальным словам, но т.к. мы этого не делаем, приходится выкручиваться (еще одна жертва богу online-моделей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \n",
    "    lmbda : float, default=0.01\n",
    "        параметр регуляризации\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16,\n",
    "                     lmbda = 0.01):\n",
    "        \n",
    "        self._loss = []\n",
    "        self._acc = []\n",
    "        n = 0\n",
    "        test_acc = 0.0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                # уникальные слова вопроса\n",
    "                unique_words = set(sentence)\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "                \n",
    "                # количество определённых тегов, количество совпавших тегов\n",
    "                identified = 0\n",
    "                overlapped = 0                \n",
    "                \n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    sigma = 1 / (1 + np.exp(-z))\n",
    "            \n",
    "            \n",
    "                    # обновляем количества определённых и совпавших тегов\n",
    "                    if sigma > 0.9:\n",
    "                        identified += 1\n",
    "                        if y == 1:\n",
    "                            overlapped += 1\n",
    "    \n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    log_arg = max(tolerance, sigma if y == 1 else 1 - sigma)\n",
    "                    sample_loss += -np.log(log_arg)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # делаем градиентный шаг\n",
    "                        # сначала проводим регуляризацию\n",
    "                        for word in unique_words:        \n",
    "                            w = self._w[tag][self._vocab[word]]\n",
    "                            self._w[tag][self._vocab[word]] -= lmbda * w\n",
    "                        \n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        dLdw = y - sigma\n",
    "                        \n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:              \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate * dLdw\n",
    "                        self._b[tag] -= -learning_rate * dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "                \n",
    "                # вычисляем точность\n",
    "                sample_acc = overlapped / (identified + len(tags) - overlapped)\n",
    "                if n >= top_n_train:\n",
    "                    test_acc += sample_acc\n",
    "                self._acc.append(sample_acc)\n",
    "        \n",
    "        # завершаем вычисление точности для тестовой выборки\n",
    "        test_acc /= (n - top_n_train)\n",
    "        return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccaaa052bc2b4aa4a3826492e41380ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.45\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 5.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4 <---\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet регуляризация, вывод\n",
    "Помимо $L_2$ регуляризации, часто используется $L_1$ регуляризация.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(\\textbf W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "Если линейно объединить $L_1$ и $L_2$ регуляризацию, то полученный тип регуляризации называется ElasticNet:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(\\textbf W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- где $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "В качестве шестого вопроса вам предлагается вывести формулу градиента ElasticNet регуляризации (не учитывая $-\\mathcal{L}$). \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$ <---\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Регуляризация ElasticNet , реализация\n",
    "\n",
    "В качестве седьмой задачи вам предлается изменить класс `LogRegressor` таким образом, чтобы метод `iterate_file` принимал два параметра со значениями по умолчанию `lmbda=0.0002` и `gamma=0.1`. Сделайте один проход по датасету с включенной `ElasticNet`-регуляризацией и заданными значениями по умолчанию и ответьте на вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \n",
    "    lmbda : float, default=0.0002\n",
    "        степень регуляризации\n",
    "    \n",
    "    gamma : float, default=0.1\n",
    "        доля l2 регуляризации\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16,\n",
    "                     lmbda = 0.0002,\n",
    "                     gamma = 0.1):\n",
    "        \n",
    "        self._loss = []\n",
    "        self._acc = []\n",
    "        n = 0\n",
    "        test_acc = 0.0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                # уникальные слова вопроса\n",
    "                unique_words = set(sentence)\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "                \n",
    "                # количество определённых тегов, количество совпавших тегов\n",
    "                identified = 0\n",
    "                overlapped = 0                \n",
    "                \n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    sigma = 1 / (1 + np.exp(-z))\n",
    "            \n",
    "            \n",
    "                    # обновляем количества определённых и совпавших тегов\n",
    "                    if sigma > 0.9:\n",
    "                        identified += 1\n",
    "                        if y == 1:\n",
    "                            overlapped += 1\n",
    "    \n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    log_arg = max(tolerance, sigma if y == 1 else 1 - sigma)\n",
    "                    sample_loss += -np.log(log_arg)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # делаем градиентный шаг\n",
    "                        # сначала проводим регуляризацию\n",
    "                        for word in unique_words:        \n",
    "                            w = self._w[tag][self._vocab[word]]\n",
    "                            self._w[tag][self._vocab[word]] -= lmbda * (gamma * 2 * w + (1 - gamma) * np.sign(w))\n",
    "                        \n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        dLdw = y - sigma\n",
    "                        \n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:              \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate * dLdw\n",
    "                        self._b[tag] -= -learning_rate * dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "                \n",
    "                # вычисляем точность\n",
    "                sample_acc = overlapped / (identified + len(tags) - overlapped)\n",
    "                if n >= top_n_train:\n",
    "                    test_acc += sample_acc\n",
    "                self._acc.append(sample_acc)\n",
    "        \n",
    "        # завершаем вычисление точности для тестовой выборки\n",
    "        test_acc /= (n - top_n_train)\n",
    "        return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef59c39a47d401bbfd58bee1d939f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.57\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 7.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.59 <---\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Самые важные слова для тега\n",
    "\n",
    "Прелесть линейных моделей в том, что они легко интерпретируемы. Вам предлагается вычислить, какие слова вносят наибольший вклад в вероятность появления каждого из тегов. А затем ответьте на контрольный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "javascript : javascript, x20, 125, ng, 3\n",
      "python : python, py, def, 00, django\n",
      "c# : xsl, writeline, binding, net, linq\n",
      "php : php, x5c, _post, echo, gt\n",
      "jquery : jquery, ajax, ready, span, val\n",
      "ios : dylib, ios, nsstring, uiview, corefoundation\n",
      "android : android, arm, imgsrv, 29297, activity\n",
      "c++ : c++, cout, std, _defaultimage, boost\n",
      "html : 3, br, html, lt, span\n",
      "java : java, hibernate, println, servlet, spring\n"
     ]
    }
   ],
   "source": [
    "model._vocab_inv = dict([(v, k) for (k, v) in model._vocab.items()])\n",
    "\n",
    "for tag in model._tags:\n",
    "    print(tag, ':', ', '.join([model._vocab_inv[k] for (k, v) in \n",
    "                               sorted(model._w[tag].items(), \n",
    "                                      key=lambda t: t[1], \n",
    "                                      reverse=True)[:5]]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 8.</font> Для многих тегов наличие самого тега в предложении является важным сигналом, у многих сам тег является самым сильным сигналом, что не удивительно. Для каких из тегов само название тега не входит в топ-5 самых важных?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. c# <---\n",
    "2. javascript\n",
    "3. jquery\n",
    "4. android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Сокращаем размер словаря\n",
    "Сейчас количество слов в словаре – 519290, если бы это была выборка из 10 миллионов вопросов с сайта StackOverflow, то размер словаря был бы миллионов 10. Регуляризировать модель можно не только изящно математически, но и топорно, например, ограничить размер словаря. Вам предоставляется возможность внести следующие изменения в класс `LogRegressor`:\n",
    "- добавить в метод `iterate_file` еще один аргумент со значением по умолчанию `update_vocab=True`\n",
    "- при `update_vocab=True` разрешать добавлять слова в словарь в режиме обучения\n",
    "- при `update_vocab=False` игнорировать слова не из словаря\n",
    "- добавить в класс метод `filter_vocab(n=10000)`, который оставит в словаре только топ-n самых популярных слов, используя данные из ``train``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # счётчик слов словаря\n",
    "        # пример: self._counter[self._vocab['exception']] = 5 означает что слово exception встречалось 5 раз\n",
    "        self._counter = Counter()\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Уменьшить размер существующего словаря до заданного, избавившись от наименее популярных слов\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    n : int, default = 10000\n",
    "        ограничение на размер словаря\n",
    "    \"\"\"\n",
    "    def filter_vocab(self, n = 10000):        \n",
    "        inv_vocab = dict([(idx, word) for (word, idx) in self._vocab.items()])\n",
    "        sorted_words = sorted(self._counter.items(), key = lambda t: t[1], reverse = True)\n",
    "        to_erase = [(idx, inv_vocab[idx]) for (idx, count) in sorted_words[n:]]\n",
    "        for idx, word in to_erase:\n",
    "            del self._vocab[word]\n",
    "            del self._counter[idx]\n",
    "            for weights in self._w.values():\n",
    "                del weights[idx]\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \n",
    "    lmbda : float, default = 0.0002\n",
    "        степень регуляризации\n",
    "    \n",
    "    gamma : float, default = 0.1\n",
    "        доля l2 регуляризации\n",
    "        \n",
    "    update_vocab : bool, default = True\n",
    "        если False, модель будет игнорировать слова, не содержащиеся в уже созданном словаре\n",
    "        \n",
    "    Возвращает\n",
    "    ---\n",
    "    test_acc : float\n",
    "        точность предсказаний на тестовой выборке\n",
    "        \n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16,\n",
    "                     lmbda = 0.0002,\n",
    "                     gamma = 0.1,\n",
    "                     update_vocab = True):\n",
    "        \n",
    "        self._loss = []\n",
    "        self._acc = []\n",
    "        n = 0\n",
    "        test_acc = 0.0\n",
    "        \n",
    "        if update_vocab:\n",
    "            self._counter.clear()\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                \n",
    "                if n >= total:\n",
    "                    break\n",
    "                \n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # обновляем словарь, если вопрос из тренировочной выборки, и словарь можно обновлять\n",
    "                # иначе фильтруем предложение согласно словаря\n",
    "                if update_vocab and n < top_n_train:\n",
    "                    for word in sentence:\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                    self._counter += Counter([self._vocab[word] for word in sentence])\n",
    "                else:\n",
    "                    sentence_temp = sentence\n",
    "                    sentence = []\n",
    "                    for word in list(sentence_temp):\n",
    "                        if word in self._vocab:\n",
    "                            sentence.append(word)\n",
    "                \n",
    "                # уникальные слова вопроса\n",
    "                unique_words = set(sentence)\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "                \n",
    "                # количество определённых тегов, количество совпавших тегов\n",
    "                identified = 0\n",
    "                overlapped = 0                \n",
    "                \n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    z = self._b[tag]\n",
    "                    for word in sentence:\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    sigma = 1 / (1 + np.exp(-z))\n",
    "            \n",
    "                    # обновляем количества определённых и совпавших тегов\n",
    "                    if sigma > 0.9:\n",
    "                        identified += 1\n",
    "                        if y == 1:\n",
    "                            overlapped += 1\n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    log_arg = max(tolerance, sigma if y == 1 else 1 - sigma)\n",
    "                    sample_loss += -np.log(log_arg)\n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # делаем градиентный шаг\n",
    "                        # сначала проводим регуляризацию\n",
    "                        for word in unique_words:        \n",
    "                            w = self._w[tag][self._vocab[word]]\n",
    "                            self._w[tag][self._vocab[word]] -= lmbda * (gamma * 2 * w + (1 - gamma) * np.sign(w))\n",
    "                        \n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        dLdw = y - sigma\n",
    "                        \n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:              \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate * dLdw\n",
    "                        self._b[tag] -= -learning_rate * dLdw\n",
    "                \n",
    "                # вычисляем точность\n",
    "                sample_acc = overlapped / (identified + len(tags) - overlapped)\n",
    "                if n >= top_n_train:\n",
    "                    test_acc += sample_acc\n",
    "                self._acc.append(sample_acc)\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "                \n",
    "                n += 1\n",
    "        \n",
    "        # завершаем вычисление точности для тестовой выборки\n",
    "        test_acc /= (n - top_n_train)\n",
    "        return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee8c36f3519432aa155375efae0b196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.57\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оставим только топ 10 000 слов\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57d7e3595334f99b44948aee12f737a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.69\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD3CAYAAAAE2w/rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAeWElEQVR4nO3deWCU1b3/8fdkZwk7si+CeBAXlhASREK04oYLxbrUrSpqUasC+eltr/V6e3/1/uqtQa9WpYLK7aJSRfuzVipalwCypCBaFU9lEwHLalgC2ef+MclkJplJJmG2Z+bz+ut5znMm881Avjn5Puecx+V2uxEREedJiXUAIiLSPkrgIiIOpQQuIuJQSuAiIg6lBC4i4lBp0XiTuro6d22tZruIiLRFenrqPqB3sOtRSeC1tW7Kyo5G461ERBJG797ZX7V0XSUUERGHUgIXEXEoJXAREYdSAhcRcSglcBERh1ICFxFxKCVwERGHivsEvuyLPZQdq451GCIicScqC3naq7yqhvv//AUApUUFMY5GRCS+xPUIvGN6KgCDu3eIcSQiIvEnrhO4y+Wib3YmZ/TvEutQRETiTlwncIAUF+w7UhXrMERE4k5c18ABdh2qZNehyliHISISd+J+BC4iIoHFfQI/tW82AG639hMXEfEV9wn8nBG9ADhWXRfjSERE4kvcJ/CuHTxl+oMVWswjIuIr7hN4dqYngR+prIlxJCIi8SXuE3jn+gR+WAlcRMRP3Cfwjhme1Zg7vq2IcSQiIvEl7hN4r04ZADy/dnuMIxERiS9xn8D7ZGcCsKNMI3AREV9xn8BdLlesQxARiUtxn8BFRCQwRyVwrcYUEWkU0mZWxpg84GFrbaExZhzwJ+DL+stPW2sXRypAX/vLq+jVOTMabyUiEvdaTeDGmPuA64Hy+qZxwDxrbXEkAwvkwl+v0ZN5RETqhVJC2QzM8DnPAaYZY0qMMc8aY7IjE1qja3IGRPotREQcp9UEbq1dAvhuRLIWuNdaWwBsAR6MUGxecwqHR/otREQcpz03MV+z1q5rOAbGhjGeVukJ9SIiHu1J4G8ZYybUH38HWNdS53Cb+tSqaL6diEjcak8Cvx14zBjzPjAJ+HlYIwri5vzB0XgbERHHcEVjbnV1da27rOzocX+d3OISAGbmD2bWpKHH/fVEROJZ797Z64Dxwa47aiFPg2dXa2MrERFHJfD/unSU9/jTbw7FMBIRkdhzVAI/u/75mAA3vbAhhpGIiMSeoxI4wIs35MQ6BBGRuOC4BH5S707e44OaEy4iScxxCdzXrD98EusQRERixpEJ/CdTRwCwaV95Kz1FRBKXIxP4jDP6eY8vmL+aiuraGEYjIhIbjkzgvvaXVzH58ZWxDkNEJOocn8BFRJKVYxP4rRMb90YZN7BrDCMREYkNxybw284cyv+/ZQL5Q7qzfsdBtuzXDU0RSS6OTeAA/btmsfqrbwG4alFUd7UVEYk5Ryfwpmpq62IdgohI1Dg+gZfcPcl7/Oon38QwEhGR6HLUfuDBLF6/k0fe2+w979Exnbdunxix9xMRiYaE3A+8qek+C3sADhzVHikikvgSIoFnpjX/NnKLS3jz890xiEZEJDoSooQC8Pddh+jeMZ3vPlvq135NzgBeWLeTK8f0597vnBTRGEREwqm1EkrCJPAGDc/NDGb53ZPISk+NSiwiIscjKWrgvhZcNZqppnfQ69o3RUQSRVqsAwi3MQO7MmZgV962e2MdiohIRCXcCLzBPVOGeY/HDOjCzPzGvVO+PVoVi5BERMIqpBq4MSYPeNhaW+jTdg1wl7W21QnX0ayBN/Xt0Sq6d8wAGuvjV4zpz326oSkice64a+DGmPuAhUCWT9sYYCbgCkOMEdWQvAF+cq4nab+8YVeswhERCZtQSiibgRkNJ8aYnsAvgNmRCipSCkf08h5vOxCbvwhERMKl1QRurV0CVAMYY1KBZ4E5wOHIhhZ+3Tqke4+fXrEtdoGIiIRBW29i5gAjgKeBl4BRxpjHwh5VhKS4XNx7jqeM8u6X+2IcjYjI8WlTArfWrrXWnlp/M/Nq4HNrraNKKVeO7d+sbf2OMv7w0c4YRCMi0n4JNw88FGeP6MV7X+5jz+FKpj2zxtteXlXLUyu2sejasZzaNzuGEYqItC7hltKHorXl9gClRQVRiEREJLikW0ofiiU357ba58X1KqmISHxLygQ+uHsHv/PLR/dr1meezwMiRETiUVImcPBP4j8+dwSlRQWUFhXw4eyzvO2//OumWIQmIhKSpLyJCfDSD3JYtPZrrh8/0K89PbXxd9ofNuzicGUN/3HRyGiHJyLSqqQdgaenpnDrxCEB9wZ/547G7V2WbtxDZY2edi8i8SdpE3hLunZI58QeHb3n0xeuDWnmiohINCXlNMJQ1Lnd5M1bHvCaphiKSDRoGmE7pbhcLJ2VH/DaoQo99V5EYk8JvAW9OmUEbP/Ok6uiHImISHNK4K144PyTYx2CiEhASTuNMFSXntaXS0/r6z3PLS4hNSXun2MhIklAI/A2Gt6rI7V1bqJx81dEpCVK4G20eZ9nNs0nuw7FOBIRSXZK4G3UNzsTgFte+pjqWi3wEZHYUQJvowVXj/YeT1+4NoaRiEiyUwJvo75dsrzHe45UkVtcwmPvb4lhRCKSrJTA2+Ftn71SAH6/bof3+HBFjW5wikhUKIG3Q7cO6XTJ8p+BmVtcwssbdnHOkx/y0NtfxigyEUkm2gvlOLz5+W4eXGoDXptTOIxrcgYGvCYiEgrthRJBF43qw+2Thga89qjq4iISYUrgx+nGvEFMO7VPrMMQkSSkBH6cUlwu/v0Cw7Lb87k5fzCv3NT4187BY55dCz/ceoBN+8pjFaKIJCjVwCMg2MMfVs+ZzJPLt9Krc4bq4yLSqtZq4CElcGNMHvCwtbbQGDMKeAZwAR8Dd1lra1t6fbIl8JLN+yn642ch9f3jLbnsO1LF6AFdIxyViDjNcd/ENMbcBywEGlaw/Cfwr9baSUBH4NIwxJlQCob3DLnv9IWl3PLSxxGMRkQSVSg18M3ADJ/zy621JcaYDKAvsDsikTncstsDP80nmG0HkucvFBEJj1YTuLV2CVDtc15rjBkCfAb0AgJPhE5y3TtmcG19nXv53ZP4/rgBLfa/4vm/RSMsEUkg7Xqgg7X2K2CEMeYWYB7wg7BGlSBmFw5jduEw73H/rllcclofOmV4PvZ1X5exv7yK+//8RSzDFBGHavM0QmPM68aYEfWnhwHtqRqCFJeLq8cN8CZvgJxB3Thv5AkxjEpEnKw9I/BfAIuMMVXAUeCW8IaUvGrr3Hpcm4iETPPA44DvvPH5V57B6P5dSEvVGiuRZKe9UBzg4UtHeY9n/eETJj62IobRiIhTKIHHgXNG9GrWpmmFItIaJfA41XRa4aptB/jLxj0A/PNQRSxCEpE4owQeJ/KHdAfgF5ec4m3LLS7hjc/+ySPvbuLuJZ/ywJtf8Nd/7OWSBWtZ+9W3sQpVROKEbmLGCbfbTXWtm4y0lKCbYTU1e8owrh2vTbFEEpVuYjqEy+UiI83zz9GtQ3pIr3nsgy3U6fmbIklLCTwOtWUflbx5y3lr4x427SvnyeVb2X24UkldJEmohBLHGkop7/3oTLLSU5n46HLAs7fKL9/dxOufBt9HbO3cybhcWhQk4mRh2Q/8eCmBt0/ZsWpSXNAlK3BJpaVa+Tt3TKRriKUYEYlPqoE7WLcO6UGTN3hWbQazr7wqEiGJSBxRAnewnEHdWHnPWd7z568Zw6xJQwC4+n/Wedtzi0vILS6hpk61cZFEogTucBlpKTz7/TG89IMcTuvXhYlDe/hdn/3qp97jhhq6iCSGdu0HLvHljP5dvMej+mZ7j0OdTy4izqQReJIpO1bdeicRcQTNQklA3xyq4NIFa73nI3p34su95c36lRYVNGv7aMdB0lJcnO4zqheR2NAslCTUr0sWM87o5z1/4YYcZuYPbtbvUEXz0fhtiz/m5hc3cPEza4jGL3cRaT+NwBPYkcoaOmakklK/oKelmvhfZuXz0vqdLFr7tbct1QWr5zYfpYtIdGgEnsQ6Z6Z5kzdA9xYW9lwwf7Vf8gaorf/dvnj9To5W1UYkxqZ2H67kSGVNVN5LxOmUwJPIsjsmAvDIZaNa7Pdv55/sPc4tLuGR9zYz5YmV1EZhHvnFz6zh7F996NfWMI9d+6CL+FMCTzKlRQVMOakXL984nnumDAvY55LT+gZsz390ObnFJSxevzPscR2uqPEr8Tz6/mYAv8VHl/jcmBUR1cCTXm2dm9sWf8wnuw5520qLCnC73UyY1/LCn0CzWAJxu91+G2tt2lvO93+zjqvG9uf/nHOS9zyQ12bm8t1nS5u1nzWsB49+9zQWrPqKzNQUbpgwKKRYRJxEm1lJqw5VVPOdJ1cBnv1VcgZ1Azyj4BfWBR9t++54+MGmfew+XMmVYwcAsP3bY1z+XPPEO7BbFjvKwl8KGdazI3MLh5M3tHvYv7ZIrCiBS0he2bCL59ds588/9N+LfF95FWVHqwOOkC87vS/XjR/o9/zOF3+Qw0m9OrV7FWjf7Eyuzx3IL9/d3K7XP/f9MZrDLgkjLAncGJMHPGytLTTGjAGeAGqBSuAGa23wjalRAk8EvmWQ2a9+ysqtB4L2LS0qaFcC9y3J1Na5yffZu+XXV53BDxd/AsCsSUOYv/KrFr/Oyq0HOLVPNt06aktdca7jnkZojLkPWAhk1Tf9N3CXtbYQeBX4l+MPU+Kdbw37p+eNaLFvRXXjlMPld0/iw9lntdAbnr7ijGb19NQUF/Omn+o9HzewG6VFBZQWFTAzf4i3vUN6Ch/cNYnB3Tt42y5/rpTZr37K1KdXtfxNiThcKLNQNgMzfM6vttZuqD9OAzS3K8n06pzZrM03AU9+fKX3OCs9lfTUFG/yfaB+iuLDl5zi7TN+cLeA7zN5eE/WzJ3MmrmTm117+46JTDqxB2/fcSYdM1JZcnOu99r2b495j19Yt6MN35mIs4RaQhkKvGStzfdpOxN4Fiiw1u5t6fUqoSQe3xLJb68by8g+2c3KJped1pef+swpb1Bb5yY1JfyPe6tzu8kLMHMm1NkyIvEmIisxjTFXAfOBaa0lb0lMS3+Yx/TT+7J27mRG9vFsYbvw6tF+fe4PUmqJRPIG/Fad+vrsn4cj8n4isdbm/cCNMdcBPwQKrbXB72RJQuvVOZP7z/MfXY8e0NXvPBYPVfYdbTf8RXDj7z/SKFwSUptKKMAkYC+wHSirv/yBtfbBll6vEkpyqaiuJSMtJeiIOFo27yv3PlruictPI7/J04pE4p3mgUtS863L3zpxMLedOTR2wYi0kXYjFKm3YNV2cotLuDbIsn0Rp1ECl4S2NsAUxH/sLafO7fabry7iRErgktBcLlfAXRfz5i1n8uMr2bpfpT1xLtXAJSnUud18sGk/973+ebNrmqEi8Uo1cBE8c8TPHtEr4LXc4hIuW6i9xsV5lMAlqSy4anTA9l0HK/QQZ3EcJXBJKmcMaNxqduks/61zV2zRujRxFiVwSSopLher50zmnTsm0qtTBj+70HivLfn4G8qravTsTXEM3cSUpOZ2u3lx/U4efX+LX/vqOZMjtmeLSKh0E1OkBS6Xi2tyBjZr932YhEi8UgIXAe4uOLFZ27YD+qtR4lubdyMUSUTXjR/IWcN6MrRHBybU7yn+z0MVDO3RMcaRiQSnEbgInlLKiT074nK5+EX904LuWvJpjKMSaZkSuEgTYwc27mueW1xCbnEJZUerYxiRSGBK4CJN9OiY0axt6tOrmj0yTiTWlMBFAlg9p/kuhgBlR6vJLS7hzc93RzkikeaUwEUCSE1xce85w8kf0t2vferTqwB4cKmlTkvvJca0kEekFc+v2U51bR0LVm1vdu2Vm8bzvef/xos35HBS704xiE4SmR6pJhIGb23cw0/f/KLVfjmDujL/ysAbZom0lVZiioTBuaa397hHx/Sg/dZ9fZB9RyqjEZKIRuAiodp7pJIDR6sxJ3Rm24GjXPH834L2/eudE+mSFTzRi4RCI3CRMOndORNzQmcAvxWa954znD/dOsGv73ee1LRDibyQRuDGmDzgYWttoU/bo4C11s5v7fUagUuyCJS09cg2aa/jHoEbY+4DFgJZ9ee9jTFLgUvDFaRIoph+et9YhyBJJJQSymZghs95Z+Dfgd9GIiARJ7v/vJN5/dYJvHFbnrdtxZb9MYxIElmrCdxauwSo9jnfaq1dE9GoRBysX5cs+mRnkp3p2exzzmufxTgiSVS6iSkSIcvumOg93n1YUwsl/JTARSIkzeeRbBc/s4aNuw/HMBpJRErgIhF0/fjGx7Xd8LuPuP+NjTGMRhKNFvKIRNDBY9Wc+9Qqv7ZrcgYwp3B4jCISJ9FCHpEY6tohnd9eN9av7YV1O5nzmp72I8dPCVwkwkb2yW7WtmLLAa3UlOOmBC4SBYtvzOG68QOZcUY/v/bc4hLesXtjFJU4nWrgIlH2/JrtPLViW7P2+6eOYHqTBC/JTTVwkThzU95gzh/Zu1n7Q29/GYNoxMmUwEVi4OfTTmHRtWObtecWl/DjP33O+h1l5BaXsFd7i0sLVEIRiaGK6lq2HTjK9b/7KGif9FQX/zntFN7cuIcHzjuZ7Ky0KEYosaRHqok4wLz3NvPi+p0h9dX2tMlDNXARB7i74MSQ+37vudIIRiJOohG4SJypqXNTdqyanWXHuOWlj0lPdVFd2/znVCPxxNfaCFzFNJE4k5biolenDHp1yvAm6cMVNWzaV85tiz/29vv7rkOc3r9LrMKUOKARuIiD7C+v4oL5q/3a1sydTIrLFeQV4mSqgYskkJ6dMni9yQOU8+Yt50hlTZu/1rYDR8ktLiG3uISjVbXhClGiSAlcxGH6dclq1tba5ljlVTUcqazh62+PsWlvOQBXPP837/UpT6wMb5ASFaqBizjQqtlnMfGxFd7zDTsP4Xa7cQUppRQ+8WG0QpMo0ghcxIHSUlMoLSrgmatGe9vm/rHtz96cdGKPgO0lm/dTstnzMGa3281L63fy8c6DIX1Nt9vNu//YS01tXZvjkbbRCFzEwcYO7Oo9XrHlgN+1x97fwu/X7eCpK04P+vr/e9FIznnSMzpvGMF/tOMgRfW/DJbOyudCn5umP7vQcNGoPi3GNGHecu/xazNzGditQ+jfkLSJRuAiDuc7H/xIZQ1//mw3ucUl/H7dDgDuePnvfv1vyhsEwNIf5vkty58wbznrd5T5TVW8sMmMlweXWv6ycQ8AdW53q6Ps7z7buOho075y9mlvl7DSNEKRBBDKwyEeOO9kLj29b7P2+9/YyLI27km+Zu5k8nxG2g2/RILFUXhST97f5CnJnGd6M3PiYIb17NSm90xGmkYokgT+eEtuwPbJwzw17stH9wuYvAEeuviUVr/+azNzmXZqY+nEN3mDp/xyqKLae96/S6bf9YbkDbDM7uWqRetafU9pnUbgIgmi6ei3LUvt3W63t3b9u+vHYU7ozM/+Ynnjs93cM2UY140fGPA9giktKqDsWDVTmzzQuanLR/ej6OzhpKemsOtgBZctXMuI3p345WWjGNBVtXONwEWSxAd3TfIet3WfFN/ph+aEzgA8eIGhtKjAm7zBMxL39avLm98gfeUmT77p1iGd1XMme9vfvfPMZn2XfPwNZ9ZPh7xs4VoAvtxbzvSF2rArFBqBiySQTXvL6ZyZSt8Ai33CadqvV9MpM40/3Di+zSP/QKP40qKCoKP71XMmk5qSnFsFhGU/cGNMHvCwtbbQGHMSsAhwA58Cd1prW7wVrQQukri27T/KMruHw5W1/GjyiWSmtfyHfU1tHXe88nc6ZaR6pz6+enMuM4Jsk3vPlGGceWL3pLzpedwJ3BhzH3A9UG6tzTfGvA7Ms9a+b4yZD7xlrX2tpa+hBC4igRQ+sZLyJvuwzD17OPPe29ysbzJunxuOGvhmYIbPeQ7wQf3xUuDcdkcnIkntnSZ18fNH9ub74wZQWlQQdJWoNGo1gVtrlwDVPk0ua23DsP0w0LX5q0REWpeW4vLeNAWYOLQxaT824zQemjbSe/7JrkNRjc0J2jMLxbfenQ2UhSkWEUlCv7t+HD8+9yQAv7nmAOeNPIF7zxkOwMqtB5q9Ntm1J4F/ZIwprD++EFjeQl8RkVZdPrp/0Bp3wfCeADy3enuzUXh5Vdv3QU8k7UngRcDPjDGrgAzglfCGJCLSyHdK5MwXN3Cksobc4hJKNu+n8IkPmf1qy3uhJzLNAxeRuJc/r4QAz3X2StQZKlqJKSKOt2rO5Bbnlx+rTs5HwimBi0jcc7lcTBzaPej19V+H9rCJRKMSiog4wp7DlUx7Zg0Ay27PZ/FHu3hlwy4OVvjfyEykckpYltIfLyVwEYmUpnuo/Pm2PE7IzgzS21lUAxeRpNIwSk8GSuAi4mgr7zkLgEe/e6q37ZtDFbEKJ6qUwEXE0TLSUigtKuCsYT29bZcuWBvDiKJHCVxEEsZ7P2rcHOvC+aupi8I9vlhSAheRhNE5M817vK+8yu/ZnXuPVJJbXML0hYkzOlcCF5GE4vsYN4DfrP2a3OISLvq15+bmzoMV/GPPkYCvraiupexodcBr8UgJXEQSSmqKizduy/OeP7F8a7M+1/52vfe4sqaOTXvLsXuOMPnxlUx9ehW1dcFLL0eravnFO18SjSnYrdE8cBFJSG63mwnz/DdLXXxjDlctWuc9X3nPWUz67xXNXrvo2rGc2jc74Nf1nXdeWlTAXzbuYcKQbvTomEFtnZv8Rxvfc9zArvzbBSczoGuHdn0PmgcuIknJ5XLxyGWjvOcLrhrNsJ6d6N+1cXfDQMkb4MnlWzlSWdPiSBw8q0MfePMLzn96NW63f/IGWL/jINMXBn7WZzhoBC4iCW3L/nIAv4ciN1292ZKmS/N9Xztr0hDmr/yqzV8jVFpKLyLSxO7DlVzss2LTN8EGSu4N1wOVZQIpLSrgm0MVfLBpPxOGdPP75dEWrSXwtGAXREQSVZ/sTJbfPYlnV2+nd+eMVvtX1dSRkZbiTd7X5AzghXU7A/ZdM9czC6ZflyyuHjcgfEEHoBq4iCSlrPRU7px8IleO9U+yv/re6QDcP3WEt+2X727iiZIt3nPf5F109nBeuGEcAA9ecDIpLlckw/ajEoqISBAvb9jFf/11U7P2t++YSFqKi6NVtRHd+VCzUERE2ul7o/s1a/t/F59Ctw7pdM5Mi/m2taqBi4gE4XK5GNQti6/LKsgZ1JXZU4Yxsk/g+eGxoBKKiEicUglFRCRBKYGLiDhUu2rgxphM4HlgGHAIuNNa+2U4AxMRkZa1dwR+K3DEWpsP3AX8KnwhiYhIKNqbwEcBSwGstRY4JWwRiYhISNqbwDcAFxtjXMaYfGCAMSY1jHGJiEgr2pvAn8NT+34PuARYZ62tDVtUIiLSqvYm8FxghbW2EHgN2NJydxERCbd2LeQxxvQCXgI6AWXATGvtrhZeshdofdNcERHxNQToHexiVFZiiohI+Gkhj4iIQymBi4g4lBK4iIhDKYGLiDiUEriIiEMpgYuIOFRSPpHHGJOOZzXpUCAT+DnwObAIcAOf4tlhsc4Y8yAwDagBZltr1xpjTgq1bzS/r+NhjDkBWAdMxRP/IpL3s/gJcCmQATwFfECSfh71Pyv/g+dnpRbPRnZJ+f/DGJMHPGytLWzL9xWOvsFiStYR+HXAfmvtZOBCPLspzgN+Wt/mAi4zxowDpgB5wNXAk/Wvb0vfuFf/Q/pr4Fh9UzJ/FoXAmcAkPN/DIJL48wAuAtKstWcC/wE8RBJ+HsaY+4CFQFZ9U6Q+g2Z9W4orWRP4y8ADPuc1QA6ekRZ4dlo8FzgLWGatdVtrtwNpxpjebezrBI8A84GG1bTJ/FmcD/wdzxYRfwLeILk/j3/giTcF6AJUk5yfx2Zghs95pD6DQH2DSsoEbq09Yq09bIzJBl4Bfgq4rLUNy1IPA13x/Ic96PPShva29I1rxpgbgb3W2rd8mpPys6jXC88zCK8AZgG/B1KS+PM4gqd88gWwAHicJPz/Ya1dgueXV4NIfQaB+gaVlAkcwBgzCM9uir+11r4A+NaZsvHs8XKo/rhpe1v6xrubganGmPeBMcBvgBN8rifTZwGwH3jLWltVv9d9Bf4/RMn2eczB83mcDIzGUw/P8LmebJ9Hg0jli0B9g0rKBG6M6QMsA/7FWvtcffNH9fVP8NTFlwMrgfONMSnGmMF4RmL72tg3rllrC6y1U+p3ltwA3AAsTcbPot4K4IL6ve7749mw7a9J/Hl8S+NI8QCQTpL+rDQRqc8gUN+gknIWCvCvQHfgAWNMQy38HuBxY0wGsBF4xVpba4xZDqzC88vuzvq+RcCCEPs6UVu+v4T6LKy1bxhjCoC1NMa+lST9PIBHgefq48/A87PzN5L382gQqZ+RZn1bCkK7EYqIOFRSllBERBKBEriIiEMpgYuIOJQSuIiIQymBi4g4lBK4iIhDKYGLiDjU/wITZTPIFH1mIQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# сделаем еще одну итерацию по датасету, уменьшив скорость обучения в 10 раз\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 9.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68 <---\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Прогнозирование тегов для новых вопросов\n",
    "\n",
    "В завершение этого задания вам предлагается реализовать метод `predict_proba`, который принимает строку, содержащую вопрос, а возвращает список предсказанных тегов вопроса с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # счётчик слов словаря\n",
    "        # пример: self._counter[self._vocab['exception']] = 5 означает что слово exception встречалось 5 раз\n",
    "        self._counter = defaultdict(int)\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Уменьшить размер существующего словаря до заданного, избавившись от наименее популярных слов\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    n : int, default = 10000\n",
    "        ограничение на размер словаря\n",
    "    \"\"\"\n",
    "    def filter_vocab(self, n = 10000):     \n",
    "        \n",
    "        inv_vocab = dict([(idx, word) for (word, idx) in self._vocab.items()])\n",
    "        sorted_words = sorted(self._counter.items(), key = lambda t: t[1], reverse = True)\n",
    "        to_erase = [(idx, inv_vocab[idx]) for (idx, count) in sorted_words[n:]]\n",
    "        \n",
    "        for idx, word in to_erase:\n",
    "            del self._vocab[word]\n",
    "            del self._counter[idx]\n",
    "            for weights in self._w.values():\n",
    "                del weights[idx]\n",
    "    \n",
    "    \"\"\"Обратить строки слов в номера\n",
    "    \"\"\"\n",
    "    def __process_sentence(self, sentence, update_vocab = False):\n",
    "        sentence = sentence.split(' ')\n",
    "        sentence_temp = sentence\n",
    "        sentence = []\n",
    "        \n",
    "        for word in sentence_temp:\n",
    "            idx = self._vocab.get(word, -1)\n",
    "            if idx == -1:\n",
    "                if not update_vocab:\n",
    "                    continue\n",
    "                idx = len(self._vocab)\n",
    "                self._vocab[word] = idx\n",
    "            sentence.append(idx)\n",
    "            if update_vocab:\n",
    "                self._counter[idx] += 1\n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \n",
    "    lmbda : float, default = 0.0002\n",
    "        степень регуляризации\n",
    "    \n",
    "    gamma : float, default = 0.1\n",
    "        доля l2 регуляризации\n",
    "        \n",
    "    update_vocab : bool, default = True\n",
    "        если False, модель будет игнорировать слова, не содержащиеся в уже созданном словаре\n",
    "        \n",
    "    Возвращает\n",
    "    ---\n",
    "    test_acc : float\n",
    "        точность предсказаний на тестовой выборке\n",
    "        \n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16,\n",
    "                     lmbda = 0.0002,\n",
    "                     gamma = 0.1,\n",
    "                     update_vocab = True):\n",
    "        \n",
    "        self._loss = []\n",
    "        self._acc = []\n",
    "        n = 0\n",
    "        test_acc = 0.0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                \n",
    "                if n >= total:\n",
    "                    break\n",
    "                \n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # фильтруем предложение согласно словаря\n",
    "                sentence = self.__process_sentence(sentence, update_vocab = update_vocab and n < top_n_train)\n",
    "                \n",
    "                # уникальные слова вопроса\n",
    "                unique_idxs = set(sentence)\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "                \n",
    "                # количество определённых тегов, количество совпавших тегов\n",
    "                identified = 0\n",
    "                overlapped = 0                \n",
    "                \n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    w_row = self._w[tag]\n",
    "                    \n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    z = self._b[tag]\n",
    "                    for idx in sentence:\n",
    "                        z += w_row[idx]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    sigma = 1 / (1 + np.exp(-z))\n",
    "            \n",
    "                    # обновляем количества определённых и совпавших тегов\n",
    "                    if sigma > 0.9:\n",
    "                        identified += 1\n",
    "                        if y == 1:\n",
    "                            overlapped += 1\n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    log_arg = max(tolerance, sigma if y == 1 else 1 - sigma)\n",
    "                    sample_loss += -np.log(log_arg)\n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # делаем градиентный шаг\n",
    "                        # сначала проводим регуляризацию\n",
    "                        for idx in unique_idxs:       \n",
    "                            w = w_row[idx]\n",
    "                            w_row[idx] -= lmbda * (gamma * 2 * w + (1 - gamma) * np.sign(w))\n",
    "                        \n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        dLdw = y - sigma\n",
    "                        \n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for idx in sentence:              \n",
    "                            w_row[idx] -= -learning_rate * dLdw\n",
    "                        self._b[tag] -= -learning_rate * dLdw\n",
    "                \n",
    "                # вычисляем точность\n",
    "                sample_acc = overlapped / (identified + len(tags) - overlapped)\n",
    "                if n >= top_n_train:\n",
    "                    test_acc += sample_acc\n",
    "                self._acc.append(sample_acc)\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "                \n",
    "                n += 1\n",
    "        \n",
    "        # завершаем вычисление точности для тестовой выборки\n",
    "        test_acc /= (n - top_n_train)\n",
    "        return test_acc\n",
    "        \n",
    "    \"\"\"Предсказать вероятность каждого тега\n",
    "    \"\"\"\n",
    "    def predict_proba(self, sentence):\n",
    "        sentence = self.__process_sentence(sentence)\n",
    "        proba = {}\n",
    "        \n",
    "        for tag in self._tags:\n",
    "            w_row = self._w[tag]\n",
    "\n",
    "            # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "            z = self._b[tag]\n",
    "            for idx in sentence:\n",
    "                z += w_row[idx]\n",
    "\n",
    "            # вычисляем вероятность наличия тега\n",
    "            sigma = 1 / (1 + np.exp(-z))\n",
    "            proba[tag] = sigma\n",
    "            \n",
    "        return proba            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435299f8358c4deda63abb5bcd6ad4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.57\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746094c01eaf44d0a5df7b770dbc43d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.69\n"
     ]
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty \" + \n",
    "            \"level\").lower().replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ios: 100.00% (36.04365338911715)\n",
      "php: 0.05% (-7.521908777093284)\n",
      "android: 0.00% (-18.84099760356188)\n",
      "java: 0.00% (-28.79240950730388)\n",
      "c++: 0.00% (-31.44236015576064)\n",
      "html: 0.00% (-48.6685643177051)\n",
      "javascript: 0.00% (-51.71521817386046)\n",
      "python: 0.00% (-54.46894880291675)\n",
      "c#: 0.00% (-56.98431052222549)\n",
      "jquery: 0.00% (-80.47930632465113)\n"
     ]
    }
   ],
   "source": [
    "for tag, p in sorted(model.predict_proba(sentence).items(), key=lambda t: t[1], reverse=True):\n",
    "    z = -np.log(1 / p - 1)\n",
    "    print('{0}: {1:.2f}% ({2})'.format(tag, 100 * p, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 10.</font> Отметьте все теги, ассоциирующиеся с данным вопросом, если порог принятия равен $0.9$. То есть считаем, что вопросу надо поставить некоторый тег, если вероятность его появления, предсказанная моделью, больше или равна 0.9. \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. android\n",
    "2. ios <---\n",
    "3. php\n",
    "4. java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "javascript : x20, 125, x30, x44, x36\n",
      "python : python, py, def, django, np\n",
      "c# : xsl, writeline, binding, linq, ienumerable\n",
      "php : x5c, _post, php, echo, laravel\n",
      "jquery : jquery, ready, sdf, ajax, val\n",
      "ios : ios, dylib, nsstring, corefoundation, uiview\n",
      "android : android, imgsrv, 29297, arm, 0x0\n",
      "c++ : c++, cout, std, _defaultimage, boost\n",
      "html : nav, h1, w3, br, padding\n",
      "java : hibernate, servlet, 176, jsp, spring\n"
     ]
    }
   ],
   "source": [
    "model._vocab_inv = dict([(v, k) for (k, v) in model._vocab.items()])\n",
    "\n",
    "for tag in model._tags:\n",
    "    print(tag, ':', ', '.join([model._vocab_inv[k] for (k, v) in \n",
    "                               sorted(model._w[tag].items(), \n",
    "                                      key=lambda t: t[1], \n",
    "                                      reverse=True)[:5]]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 0), ('ve', 1), ('got', 2), ('some', 3), ('code', 4), ('in', 5), ('window', 6), ('scroll', 7), ('that', 8), ('checks', 9), ('if', 10), ('an', 11), ('element', 12), ('is', 13), ('visible', 14), ('then', 15), ('triggers', 16), ('another', 17), ('function', 18), ('however', 19)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model._vocab.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loan', 6319, 79),\n",
       " ('257', 6735, 79),\n",
       " ('grabbed', 6885, 79),\n",
       " ('xhttp', 6995, 79),\n",
       " ('processors', 7056, 79),\n",
       " ('driverclassname', 7493, 79),\n",
       " ('textfields', 8223, 79),\n",
       " ('sourcetype', 8572, 79),\n",
       " ('wstring', 8870, 79),\n",
       " ('dn', 8975, 79),\n",
       " ('directoryentry', 8976, 79),\n",
       " ('monkey', 9745, 79),\n",
       " ('noactionbar', 9806, 79),\n",
       " ('differentiate', 9925, 79),\n",
       " ('mavencentral', 9952, 79),\n",
       " ('mycontroller', 11197, 79),\n",
       " ('particles', 11901, 79),\n",
       " ('codecs', 12227, 79),\n",
       " ('jumping', 13083, 79),\n",
       " ('autoopen', 14634, 79)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_vocab = dict((idx, word) for (word, idx) in model._vocab.items())\n",
    "words = [(inv_vocab[idx], idx, count) for (idx, count) in model._counter.items()]\n",
    "words = sorted(words, key = (lambda triple: triple[2]), reverse = True)\n",
    "words[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "javascript -> 30.5744121232201\n",
      "python -> 57.410803380197436\n",
      "c# -> 0\n",
      "php -> 31.66435014169925\n",
      "jquery -> 58.964912433176636\n",
      "ios -> 27.898669526537265\n",
      "android -> 35.95159060103402\n",
      "c++ -> 34.45160203980468\n",
      "html -> 8.136518699850908\n",
      "java -> 13.122271458043217\n"
     ]
    }
   ],
   "source": [
    "for tag in model._tags:\n",
    "    if tag in model._vocab:\n",
    "        print('{0} -> {1}'.format(tag, model._w[tag][model._vocab[tag]]))\n",
    "    else:\n",
    "        print('{0} -> {1}'.format(tag, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "javascript love = 0.000% (-51.71521817386045)\n",
      "python love = 0.000% (-54.468948802916735)\n",
      "c# love = 0.000% (-56.98431052222548)\n",
      "php love = 0.054% (-7.521908777093287)\n",
      "jquery love = 0.000% (-80.47930632465112)\n",
      "ios love = 100.000% (36.23939353752381)\n",
      "android love = 0.000% (-18.840997603561892)\n",
      "c++ love = 0.000% (-31.442360155760625)\n",
      "html love = 0.000% (-48.66856431770509)\n",
      "java love = 0.000% (-28.79240950730389)\n"
     ]
    }
   ],
   "source": [
    "for tag in model._tags:\n",
    "    z = np.sum([model._w[tag].get(model._vocab.get(word, -1), 0) for word in sentence.split(' ')]) + model._b[tag]\n",
    "    p = 1 / (1 + np.exp(-z))\n",
    "    print('{0} love = {1:.3f}% ({2})'.format(tag, 100 * p, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
